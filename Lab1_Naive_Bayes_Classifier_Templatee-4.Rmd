---
editor_options:
  markdown:
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Probability and Statistics

# Lab Assignment 1: Naive Bayes Classifier

## Work breakdown

-   *Name1 Surname1*: Viktor Syrotiuk
-   *Name2 Surname2*: Olena Dovbenchuk
-   *Name3 Surname3*: Maksym Prokopets

## Introduction

During the first three weeks, you learned a couple of essential notions
and theorems, and one of the most important among them is the **Bayes
theorem**.

**Naive Bayes Classifier** is a simple algorithm, which is based on
**Bayes theorem** and used for solving classification problems.
**Classification problem** is a problem in which an observation has to
be classified in one of the $n$ classes based on its similarity with
observations in each class.

It is a **probabilistic classifier**, which means it predicts based on
the probability of an observation belonging to each class. To compute
it, this algorithm uses **Bayes' formula,** that you probably already
came across in **Lesson 3:**
$$\mathsf{P}(\mathrm{class}\mid \mathrm{observation})=\frac{\mathsf{P}(\mathrm{observation}\mid\mathrm{class})\mathsf{P}(\mathrm{class})}{\mathsf{P}(\mathrm{observation})}$$

Under the strong **independence** assumption, one can calculate
$\mathsf{P}(\mathrm{observation} \mid \mathrm{class})$ as
$$\mathsf{P}(\mathrm{observation}) = \prod_{i=1}^{n} \mathsf{P}(\mathrm{feature}_i), \qquad \mathsf{P}(\mathrm{observation} \mid \mathrm{class}) = \prod_{i=1}^{n} \mathsf{P}(\mathrm{feature}_i \mid \mathrm{class}),$$
where $n$ is the total number of features describing a given observation
(*For example, if an observation is presented as a sentence, then each
word can be a feature*). Thus,
$\mathsf{P}(\mathrm{class}|\mathrm{observation})$ now can be calculated
as

$$\mathsf{P}(\mathrm{class} \mid \mathrm{\mathrm{observation}}) = \mathsf{P}(\mathrm{class})\times \prod_{i=1}^{n}\frac{\mathsf{P}(\mathrm{feature}_i\mid \mathrm{class})}{\mathsf{P}(\mathrm{feature}_i)}\tag{1}$$

All the terms on the right-hand side can be estimated as corresponding
relative frequencies using available data\

**See [*this
link*](https://www.javatpoint.com/machine-learning-naive-bayes-classifier)
for more detailed explanations & examples :) Also you can watch [*this
video*](https://youtu.be/O2L2Uv9pdDA?si=-ohkHVDuu3sLLGMq) for more
examples!**

## Data description

There are 5 datasets uploaded on the cms (data.zip)

To determine your variant, take your team number from the list of teams
on cms and take *mod 5* - this is the number of your data set.

-   **0 - authors** This data set consists of citations of three famous
    writers: Edgar Alan Poe, Mary Wollstonecraft Shelley and HP
    Lovecraft. The task with this data set is to classify a piece of
    text with the author who was more likely to write it.

-   **1 - discrimination** This data set consists of tweets that have
    discriminatory (sexism or racism) messages or of tweets that are of
    neutral mood. The task is to determine whether a given tweet has
    discriminatory mood or does not.

-   **2 - fake news** This data set contains data of American news: a
    headline and an abstract of the article. Each piece of news is
    classified as fake or credible. The task is to classify the news
    from test.csv as credible or fake.

-   **3 - sentiment** All the text messages contained in this data set
    are labeled with three sentiments: positive, neutral or negative.
    The task is to classify some text message as the one of positive
    mood, negative or neutral.

-   **4 - spam** This last data set contains SMS messages classified as
    spam or non-spam (ham in the data set). The task is to determine
    whether a given message is spam or non-spam.

Each data set consists of two files: *train.csv* and *test.csv*. The
first one is used to find the probabilities of the corresponding classes
and the second one is used to test your classifier afterwards. Note that
it is crucial to randomly split your data into training and testing
parts to test the classifier ºs possibilities on the unseen data.

```{r}
# here goes a list of recommended libraries,
# though you may install other ones if they are needed
library(tidytext)
library(readr)
library(dplyr)
library(ggplot2)
library(stringr)
```

## Outline of the work

1.  **Data pre-processing** (includes removing punctuation marks and
    stop words, representing each message as a bag-of-words)
2.  **Data visualization** (it's time to plot your data!)
3.  **Classifier implementation** (using the training set, calculate all
    the conditional probabilities in formula (1))
4.  **Measurements of effectiveness of your classifier** (use the
    results from the previous step to predict classes for messages in
    the testing set and measure the accuracy, precision and recall, F1
    score metric etc)
5.  **Conclusions**

*!! do not forget to submit both the (compiled) Rmd source file and the
.html output !!*

## Data pre-processing

-   Read the *.csv* data files.
-   –°lear your data from punctuation or other unneeded symbols.
-   Clear you data from stop words. You don't want words as is, and, or
    etc. to affect your probabilities distributions, so it is a wise
    decision to get rid of them. Find list of stop words in the cms
    under the lab task.
-   Represent each test message as its bag-of-words. Here:
    <https://machinelearningmastery.com/gentle-introduction-bag-words-model/>
    you can find general introduction to the bag-of-words model and
    examples on to create it.
-   It is highly recommended to get familiar with R dataframes, it would
    make the work much easier to do.
-   Useful links:
    -   <https://steviep42.github.io/webscraping/book/bagofwords.html#tidytext> -
        example of using *tidytext* to count frequencies of the words.
    -   Basics of Text Mining in R:
        <http://rstudio-pubs-static.s3.amazonaws.com/256588_57b585da6c054349825cba46685d8464.html>
        . Note that it also includes an example on how to create a bag
        of words from your text document.

```{r}
list.files(getwd())

list.files("data/1-discrimination")

```

```{r}

test_path1  <- "data/1-discrimination/test.csv"
train_path1 <- "data/1-discrimination/train.csv"

stop_words <- read_file("stop_words.txt")
# https://stackoverflow.com/questions/27195912/why-does-strsplit-return-a-list
splitted_stop_words <- strsplit(stop_words, split='\n')
splitted_stop_words <- splitted_stop_words[[1]]
splitted_stop_words <- trimws(splitted_stop_words)
```

```{r}
train <-  read.csv(file = train_path1, stringsAsFactors = FALSE)
test <-  read.csv(file = test_path1, stringsAsFactors = FALSE)
```

```{r}
tidy_text <- train %>%
  mutate(tweet = str_to_lower(tweet)) %>%
  mutate(tweet = str_replace_all(tweet, "user", " ")) %>%
  mutate(tweet = str_replace_all(tweet, "amp|\\bs\\b|\\bt\\b", " ")) %>%
  mutate(tweet = str_replace_all(tweet, "[^a-z\\s]", " ")) %>%
  mutate(tweet = str_replace_all(tweet, "\\s+", " ")) %>%
  unnest_tokens(word, tweet, token = "words") %>%
  filter(!word %in% splitted_stop_words,
         word != "",
         !is.na(word),
         str_length(word)>1)  

word_counts <- tidy_text %>%
  count(word, sort = TRUE)
```

## Data visualization

Each time you work with some data, you need to understand it before you
start processing it. R has very powerful tools to make nice plots and
visualization. Show what are the most common words for negative and
positive examples as a histogram, word cloud etc. Be creative!

```{r}
top_words <- tidy_text %>%
  group_by(label, word) %>%
  summarise(word_count = n(), .groups = 'drop') %>%
  group_by(label) %>%
  slice_max(word_count, n = 20) %>%
  ungroup()

head(top_words)


```

```{r}
ggplot(top_words, aes(x = reorder(word, word_count), y = word_count, fill = label)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~label, scales = "free_y") +
  coord_flip() +
  labs(x = "Words", y = "Frequency", title = "Top Words by label") +
  theme_minimal() +
  theme(strip.text = element_text(size = 12))

```

```{r}
ggplot(top_words, aes(x = 1, y = word_count, label = word, size = word_count, color = label)) +
  geom_text(check_overlap = TRUE) +
  facet_wrap(~label, scales = "free") +
  theme_void() +
  labs(title = "Word Cloud by label") +
  scale_size_continuous(range = c(4, 12))

```

## Classifier implementation

```{r}
head(tidy_text, 20)
```

```{r}
naiveBayes <- setRefClass("naiveBayes",
  fields = list(
    word_probs = "list",
    class_probs = "numeric",
    classes = "character",
    vocab = "character"
  ),
  
  methods = list(
    # Train the Naive Bayes classifier
    fit = function(X, y) {
      classes <<- as.character(unique(y))
      class_counts <- table(y)
      class_probs <<- as.numeric(class_counts / sum(class_counts))
      names(class_probs) <<- names(class_counts)
      vocab <<- unique(X$word)
      word_probs <<- list()
      
      # Calculate word probabilities for each class
      for(cl in classes) {
        words_cl <- X %>% filter(label == cl)
        word_counts <- table(words_cl$word)
        total <- sum(word_counts)
        probs <- as.numeric(word_counts) / total
        names(probs) <- names(word_counts)
        word_probs[[cl]] <<- probs
      }
    },
    
    # Predict class for a single message
    predict = function(message) {
      # Tokenize and preprocess the input message (same pipeline as training)
      tokens <- data.frame(tweet = message, stringsAsFactors = FALSE) %>%
        mutate(tweet = str_to_lower(tweet)) %>%
        mutate(tweet = str_replace_all(tweet, "user", " ")) %>%
        mutate(tweet = str_replace_all(tweet, "amp|\\bs\\b|\\bt\\b", " ")) %>%
        mutate(tweet = str_replace_all(tweet, "[^a-z\\s]", " ")) %>% 
        mutate(tweet = str_replace_all(tweet, "\\s+", " ")) %>% 
        unnest_tokens(word, tweet, token = "words") %>%
        filter(!word %in% splitted_stop_words,
               word != "",
               !is.na(word),
               str_length(word) > 1)
      
      words <- tokens$word
      
      if(length(words) == 0) {
        return(names(which.max(class_probs)))
      }
      
      log_probs <- numeric(length(classes))
      names(log_probs) <- classes
      
      for(cl in classes) {
        log_prob <- log(class_probs[cl])
        probs <- word_probs[[cl]]
        
        # Multiply word probabilities (sum logs for numerical stability)
        for(w in words) {
          if(w %in% names(probs)) {
            log_prob <- log_prob + log(probs[w])
          } else {
            # Use small epsilon for unknown words to avoid log(0)
            log_prob <- log_prob + log(.Machine$double.eps)
          }
        }
        
        log_probs[cl] <- log_prob
      }
      
      return(names(which.max(log_probs)))
    },
    
    # Evaluate model performance on test data
    score = function(X_test, y_test) {
      # Generate predictions for all test messages
      predictions <- sapply(X_test$tweet, function(msg) predict(msg))
      predictions <- as.character(predictions)
      y_test <- as.character(y_test)
      
      # Create confusion matrix
      cm <- table(Predicted = predictions, Actual = y_test)
      
      # Identify positive and negative classes
      class_counts <- table(y_test)
      pos_class <- names(which.min(class_counts))
      neg_class <- names(which.max(class_counts))
      
      TP <- ifelse(pos_class %in% rownames(cm), cm[pos_class, pos_class], 0)
      FP <- ifelse(pos_class %in% rownames(cm), cm[pos_class, neg_class], 0)
      FN <- ifelse(neg_class %in% rownames(cm), cm[neg_class, pos_class], 0)
      TN <- ifelse(neg_class %in% rownames(cm), cm[neg_class, neg_class], 0)
      
      accuracy <- (TP + TN) / sum(cm)
      precision <- ifelse((TP + FP) > 0, TP / (TP + FP), 0)
      recall <- ifelse((TP + FN) > 0, TP / (TP + FN), 0)
      F1 <- ifelse((precision + recall) > 0, 2 * precision * recall / (precision + recall), 0)
      
      error_details <- data.frame(
        tweet = X_test$tweet,
        predicted = predictions,
        actual = y_test,
        stringsAsFactors = FALSE
      )
      
      return(list(
        confusion_matrix = cm,
        accuracy = accuracy,
        precision = precision,
        recall = recall,
        F1 = F1,
        predictions = predictions,
        error_details = error_details,
        pos_class = pos_class,
        neg_class = neg_class
      ))
    }
  )
)

# Preprocess training data
tidy_train <- train %>%
  mutate(tweet = str_to_lower(tweet)) %>%
  mutate(tweet = str_replace_all(tweet, "user", " ")) %>%
  mutate(tweet = str_replace_all(tweet, "amp|\\bs\\b|\\bt\\b", " ")) %>%
  mutate(tweet = str_replace_all(tweet, "[^a-z\\s]", " ")) %>%
  mutate(tweet = str_replace_all(tweet, "\\s+", " ")) %>%
  unnest_tokens(word, tweet, token = "words") %>%
  filter(!word %in% splitted_stop_words,
         word != "",
         !is.na(word),
         str_length(word)>1) %>%
  select(word, label)

# Preprocess test data
tidy_test <- test %>%
  mutate(tweet = str_to_lower(tweet)) %>%
  mutate(tweet = str_replace_all(tweet, "user", " ")) %>%
  mutate(tweet = str_replace_all(tweet, "amp|\\bs\\b|\\bt\\b", " ")) %>%
  mutate(tweet = str_replace_all(tweet, "[^a-z\\s]", " ")) %>%
  mutate(tweet = str_replace_all(tweet, "\\s+", " ")) %>%
  unnest_tokens(word, tweet, token = "words") %>%
  filter(!word %in% splitted_stop_words,
         word != "",
         !is.na(word),
         str_length(word)>1)

# Train model on ALL training data
model <- naiveBayes$new()
model$fit(tidy_train, train$label)

# Test on REAL test set (test.csv)
results <- model$score(test, test$label)

cat("TEST SET RESULTS (test.csv)\n")
cat(sprintf("Accuracy: %.3f\n", results$accuracy))
cat(sprintf("Precision: %.3f\n", results$precision))
cat(sprintf("Recall: %.3f\n", results$recall))
cat(sprintf("F1: %.3f\n", results$F1))

failure_cases <- results$error_details[results$error_details$predicted != results$error_details$actual, ]
```

## Measure effectiveness of your classifier

-   Note that accuracy is not always a good metric for your classifier.
    Look at precision and recall curves, F1 score metric.

    When evaluating the model, it's important to understand the
    different types of classification results:

    -   A ***true positive*** result is one where the model correctly
        predicts the positive class.
    -   A ***true negative*** result is one where the model correctly
        predicts the negative class.
    -   A ***false positive*** result is one where the model incorrectly
        predicts the positive class when it is actually negative.
    -   A ***false negative*** result is one where the model incorrectly
        predicts the negative class when it is actually positive.

    Precision measures the proportion of true positive predictions among
    all positive predictions made by the model.

    $$
    Precision = \frac{TP}{TP+FP}
    $$

    Recall, on the other hand, measures the proportion of true positives
    identified out of all actual positive cases.

    $$
    Recall = \frac{TP}{TP+FN}
    $$

    F1 score is the harmonic mean of both precision and recall.

    $$
    F1 = \frac{2\times Precision \times Recall}{Precision + Recall}
    $$

    **See [this
    link](https://cohere.com/blog/classification-eval-metrics) to find
    more information about metrics.**

### Failure Cases

```{r}
errors <- results$error_details[results$error_details$predicted != results$error_details$actual, ]
classes <- unique(c(results$error_details$predicted, results$error_details$actual))

FP <- sum(results$error_details$predicted == classes[1] & results$error_details$actual == classes[2])
FN <- sum(results$error_details$predicted == classes[2] & results$error_details$actual == classes[1])

cat(sprintf("Errors: %d/%d cases (%.1f%%) - False Positives: %d, False Negatives: %d\n", 
            nrow(errors), nrow(results$error_details),
            nrow(errors)/nrow(results$error_details)*100, FP, FN))
```

### Visualization

```{r}
metrics <- data.frame(
  Metric = c("Accuracy", "Precision", "Recall", "F1"),
  Value = c(results$accuracy, results$precision, results$recall, results$F1)
)

ggplot(metrics, aes(x = Metric, y = Value, fill = Metric)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = sprintf("%.3f", Value)), vjust = -0.5) +
  ylim(0, 1) +
  ggtitle("Model Performance Metrics")

conf_matrix <- as.data.frame(results$confusion_matrix)
ggplot(conf_matrix, aes(x = Actual, y = Freq, fill = Predicted)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_text(aes(label = Freq), position = position_dodge(width = 0.9), vjust = -0.5) +
  ggtitle("Confusion Matrix")

# Top words visualization - corrected sorting
top_words <- tidy_test %>%
  count(word, sort = TRUE) %>%
  head(20) %>%
  arrange(n)  # Sort by frequency for proper bar order

ggplot(top_words, aes(x = reorder(word, n), y = n)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  ggtitle("Top 20 Most Frequent Words") +
  xlab("Word") +
  ylab("Frequency") +
  theme_minimal()

# Error analysis plot
error_details <- results$error_details
misclassified <- error_details[error_details$predicted != error_details$actual, ]

error_counts <- data.frame(
  Type = c("Correct", "Misclassified"),
  Count = c(nrow(error_details) - nrow(misclassified), nrow(misclassified))
)

ggplot(error_counts, aes(x = Type, y = Count, fill = Type)) +
  geom_bar(stat = "identity") + 
  geom_text(aes(label = Count), vjust = -0.5) +
  ggtitle("Classification Results")
```

## Conclusions

In this work, we implemented a Naive Bayes classifier for text
classification. The method is grounded in Bayes‚Äô theorem, which gives
the conditional probability of a class given an observation:

$$\mathsf{P}(\mathrm{class}\mid \mathrm{observation})=\frac{\mathsf{P}(\mathrm{observation}\mid\mathrm{class})\mathsf{P}(\mathrm{class})}{\mathsf{P}(\mathrm{observation})}$$
Under the naive independence assumption, we treat all features (words)
as conditionally independent given the class. This allows us to compute
the likelihood of a message as a product of single-word probabilities,
which can be estimated from data. Our implementation followed this
probabilistic reasoning step by step:

- The fit method corresponds to estimating the prior probabilities ùëÉ (
class ) P(class) from the class frequencies and the likelihood
probabilities ùëÉ ( word ‚à£ class ) P(word‚à£class) from relative word
frequencies in each class. This is essentially constructing an empirical
probability distribution.

- The predict method applies Bayes‚Äô theorem to a new message. It tokenizes
the text into words (features) and computes the posterior probability ùëÉ
( class ‚à£ message ) P(class‚à£message) by multiplying the prior with the
conditional probabilities of the observed words. For numerical
stability, this multiplication is performed as a sum of logarithms,
which is equivalent in probability theory to adding log-likelihoods.

- The score method evaluates predictions against the true labels by
constructing a confusion matrix. This matrix allows us to calculate
standard measures such as accuracy, precision, recall, and F1 score,
which in probability theory are related to conditional probabilities of
correct and incorrect predictions (for example, precision is an estimate
of ùëÉ ( true¬†positive ‚à£ predicted¬†positive )
P(true¬†positive‚à£predicted¬†positive)).

In this way, every part of the Naive Bayes class directly reflects
concepts from probability theory: priors, likelihoods, posteriors, and
evaluation through conditional probabilities.
    
## Pros and Cons of Naive Bayes

**Advantages:**

- Very simple and efficient to implement, even on large datasets.
- Works well with high-dimensional data like text, because it reduces the problem to counting word frequencies.
- Requires relatively little training data compared to more complex models.

**Limitations:**

- The independence assumption is rarely true in real language, since words are often correlated (e.g., ‚Äúnot good‚Äù has a very different meaning than the independent probabilities of ‚Äúnot‚Äù and ‚Äúgood‚Äù).
- It does not capture word order or context (bag-of-words model).
- Zero-probability problems may occur with unseen words, which we mitigated by using a very small epsilon.
    
## Why Accuracy Is Not Enough

While accuracy measures the proportion of all correct predictions, it can be misleading in imbalanced datasets.  
For example, if 90% of tweets are neutral, a model that always predicts ‚Äúneutral‚Äù would still have 90% accuracy ‚Äî even though it learned nothing useful.

This is why **precision**, **recall**, and **F1 score** are important:

- **Precision** focuses on the reliability of positive predictions  
  \( Precision = \dfrac{TP}{TP + FP} \)
- **Recall** measures the ability to detect all actual positives  
  \( Recall = \dfrac{TP}{TP + FN} \)
- **F1 score** combines both into a single balanced metric  
  \( F1 = \dfrac{2 \times Precision \times Recall}{Precision + Recall} \)

For classification tasks like discrimination detection, where both false negatives and false positives are critical, the **F1 score** is a much more informative metric than accuracy alone.
